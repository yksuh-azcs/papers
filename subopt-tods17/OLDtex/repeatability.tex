\section{Repeatability}
We ensure the validity of our experiment results examining the
{\em repeatability} of our experiments. 
The repeatability study focuses on three factors, namely
{\em data repeatability}, {\em within-run repeatability},
and {\em across-run repeatability}, respectively.

First, repeatability is examined within data. Although the data 
that is populated for experiment tables are randomly generated, we ensure the
same numbers are generated each time.
Second, the query plans produced at the same cardinality for the same query
should be identical. Finally, as the cardinality of the variable-table
reduces, change points, where a different plan is detected, are identified. 
The experiment is repeatable if the same set of change points, which appears 
at the same cardinalities across multiple experiment runs, can be identified. 

\shorten{
We use the following query $Q$ as an example to explain repeatability.

\begin{center}
\begin{tabular}{rl}
$Q$:	& {\tt SELECT t2.id4, t3.id1} \\
		& {\tt FROM HT2 t3, HT1 t0, HT4 t2, HT3 t1} \\
		& {\tt WHERE (t3.id4=t0.id1 AND }  \\
		& {\tt t0.id1=t2.id1 AND t2.id1=t1.id4)} \\
\end{tabular}
\end{center}
}
\shorten{
We first obtain a plan at 1M rows of {\tt HT1} (1M plan). 
Then, we find different plans, deleting the end of 10K rows 
from {\tt HT1} initially loaded with 2M rows (max) 
until the minimum cardinality which is 1M rows. 
All change points are also recorded with the plans.

In the second pass, $Q$ gets executed at all change points including 
the first 1M rows. 
We double-check whether the plan found at the change point is equal to one 
obtained at the time of execution. 
The executions for $Q$ are later used for telling the suboptimality of $Q$ 
by comparing the execution time of the 1M plan with that of other plans.

Interestingly, what we found was that in the first pass we happened to 
observe two different plans at 1M rows of {\tt HT1}. 
That is, the plans at 1M rows were inconsistent with each other 
even though all the involved tables' cardinalities were the same. 
}

\shorten{
The repeatability issue is initially raised from an experimental scenario, 
termed {\em Adjacent}, which studies query sub-optimality. 
(For more details about query suboptimality, please refer to 
Section~\ref{suboptimality}.)
The scenario includes data definition, query definition, and 
query plan detection and execution strategy. 
By data definition, we populate a few tables with fixed cardinality, 
saying 1M rows, and use a variable-table to get cardinality changing. 
Queries can be generated or pre-defined by query definition. 
For instance, let us consider the following query $Q$.}

This query involves four distinct tables, namely {\tt HT1}, {\tt HT2},
{\tt HT3}, and {\tt HT4}, which are all configured with the same sets of
columns, namely ({\tt id1}$\sim${\tt id4}). These four tables participate
in three joins, each joining a distinct pair of tables.
Tables {\tt HT2}, {\tt HT3}, and {\tt HT4} have the same fixed cardinality,
which is 1M rows. We term these tables the {\em fixed}-tables.
On the other hand, {\tt HT1} is termed a {\em variable}-table in that we
vary its cardinality from 2M rows down to 10K rows, at a step of 10K rows
each time.

\subsection{Data Repeatability}
To populate these tables, we utilize a random number generator. The values
for column {\tt id1} across the four tables are sequentially assigned.
The values for the other columns are populated by the generator. We control
the seed for the number generation such that the produced sequence of
``random'' numbers are identical each time. To change the cardinality of
the variable-table, we populate this table to the maximal cardinality,
which is 2M rows, and then {\tt DELETE} from the end of the table 10K rows
at a time.

\subsection{Within-Run Repeatability}
To study suboptimality in query evaluation, we define an experiment scenario
to carry out the experiment in an automated and structured fashion.
The scenario contains two execution stages.
In the first stage, we initially populate the variable table to have 1M rows,
which is the same to the fixed tables. We utilize the {\tt EXPLAIN} command
to produce the execution plan for $Q$ and then execute the query to measure
the running time. This measurement is used as a reference point for further
study.
We then re-populate the variable table to the maximal of 2M rows, and we delete
10K rows from the end of the table each time and call {\tt EXPLAIN} to
collect the plans. As the cardinality is reduced, if the newly generated plan
is different from the previous one, we note the current cardinality as a
{\em change point} and we store the cardinality as well as the plan. At the
end of the first stage, we usually collect a list of change points.

In the second stage, we re-populate the variable table to the maximal
cardinality again. According to the list of change points, we can directly
delete a number of rows from the variable table to achieve the target
cardinality at which a change point was found in the previous stage.
We run the query at the point to measure the running time. To ensure
that the current query execution utilizes the same time as the corresponding
change point, we compare the two plans.

An interesting observation is that sometimes in the two stages, a change
point is identified at cardinality 1M. The occurring plan at this change point
is different from the the reference plan initially generated. The reason
is that by deleting rows from a table, the deleted rows are simply flagged
without being physically removed. This leads to the issue that the number of
occupied pages, which is a factor considered by the cost model employed by
the optimizers, does not tightly correlate with the number of rows in the
table.

\shorten{
One of the reasons we speculated was that a way of setting the cardinality 
for the variable table might cause the optimizer to choose different 
plans in spite of the same cardinalities. 
To vary the cardinality of {\tt HT1}, a table with the same definition 
as {\tt HT1}, called {\tt max$\_$HT1}, is populated to the max cardinality, 
2M rows. 
To get the first 1M plan, the 1M rows of {\tt HT1} were copied from {\tt max$\_$HT1}. 
On the contrary, the second 1M plan was acquired due to the 10K-row-deletion. 

To see whether the plan difference, or unrepeatability reappeared even after 
making cardinality by a consistent method, 
we also tried other variety of scenarios: {\it deleteTop10K}, {\it doubleTriple}, 
{\it copy1M}. 
In {\it deleteTop10K} scenario, {\tt HT1} is first loaded with 2M rows 
by the max table copy, and 10K rows from {\tt HT1} are incrementally 
deleted until 1M rows. 
A 1M plan is eventually acquired at the minimum cardinality as before. 
We repeat these steps so that both of the 1M plans can be compared 
for repeatability check. 

{\it DoubleTriple} scenario basically extends {\it deleteTop10K}. 
Four plans are obtained by running {\tt deleteTop10K} scenario twice. 
Thereafter, instead of the incremental deletion, we get a 1M plan right after 
copying 1M rows into {\tt HT1} from the max table as before. 
By repeating this, the total six plans are obtained. 
We ensure repeatability by checking whether all of the six 1M plans are identical. 
}

To address the issue that the number of pages and the number of rows are not
correlated, we designed a new scenario, essentially with a different
cardinality-varying mechanism. We term this new scenario {\em copy1M}.
Unlike deleting rows, the alternative mechanism creates a new table and
utilizing the {\tt SELECT ... INTO ...} command to populate the newly created
table to the target cardinality. This approach effectively ensures that
the cardinality of the table is coupled with the number of pages the table
occupies.

\subsection{Across-Run Repeatability}
While the within-run repeatability is ensured, we run each experiment multiple
times to collect running time samples for statistical analysis. A change point
can be uniquely identified by the query ID and the cardinality value within
a run. Hence by grouping the change points with the same query ID and
cardinality value across multiple runs, should the samples be correctly
collected for each change point.

Nonetheless, we observed that for the same query, different sets of change
points are found for various runs. We utilized one DBMS to conduct a simple
test, that is to invoke the {\tt EXPLAIN} facility with the same query
at different times, which was often one minute apart or hours apart. The
underlying tables were of course not changed whatsoever. We found that
the produced plans at various times could actually be different even though
the tables stay unchanged. We concluded that this plan inconsistency issue
was due to the randomness introduced by the heuristics adopted by
DBMSes to compute the table stats.

Given that it is difficult to ensure the across-run repeatability,
we altered the approach to collecting multiple running-time samples.
Instead of execute multiple runs for each experiment, each plan, once
identified, is executed multiple times such that the samples are guaranteed
to be collected on the same plan.

In fact,\todo{is this true? I don't have facts to back this up...} the
within-run repeatability was sometimes violated due to a similar
reason. In the adjacent scenario, two stages (passes) are made to identify
and execute query plans, respectively. However, the plan produced in the
second pass could be different from the first pass.

To address the randomness caused by the optimizer, we restrict that
the optimizer only produces plan once for the same query at the same
cardinality. We term this new approach {\em onePass}. The resulting
new scenario is termed accordingly. In this scenario, instead of making
two passes of varying the cardinality of the variable table, the cardinality
is only varied once such that once a change point is identified, the generated
plan is executed multiple times. This scenario finally resolved the
repeatability violations in our experiments.
\todo{Do we talk about the two-table settings here?}

\input{onepass.tex}

\subsection{Summary}
In summary, we learned various tricky situations where repeatability
could be violated during experiments and designed the one-pass scenario
to address these issues. Moreover, we experienced great variance in
running-time measurements due to system daemons and cron jobs. We had most
of the unnecessary processes turned off, which effectively reduce the
variance in measurements. We consider this also an effort to ensure the
repeatability for the experiments, in terms of running-time sample collection.

\todo{Young mentioned exhaustive before, but I now think it should go to the monotonicity section.}
\todo{Rick and Young agreed with Rui about this.}
\shorten{
We first applied these scenarios to DBMS A through E with a few runs of 100 queries. 
As a result, DBMSes A and B showed the repeatability for all queries. 
In particular, they were not affected by which of the above scenarios 
was run or how many times the scenario was run. 
On the contrary, the other DBMSes C, D and E failed to retain repeatability. 
They were sensitive to queries, runs and scenarios. 

As a last check, we also took advantage of CLI (Command-Line Interface) on 
the unrepeatable DBMSes in order to see directly the 1M plan without passing 
through the scenarios. 
DBMS C showed the same plan at a cardinality for a query at all times. 
However, the other two DBMSes D and E produced different plans for the same 
query at times.

The empirical observation about repeatability can be summarized as follows. 
Some heuristic algorithms may be exploited for the optimizers of DBMS 
D and E, which may affect the enumeration and decision of query plan. 
(To figure out this, we made several attempts to look for {\tt heuristic} keyword 
from the source code of DBMS D and E optimizers. 
However, it was not clear of where it was used.) 
DBMS C has relatively plenty of operators compared with other DBMSes, so 
its compatible operators may show up at different times, considering that plans 
built by such operators would have the same execution cost. 
In the meantime, DBMS A and B were repeatable.

Based on the observation from all the DBMSes, we eventually ensure 
repeatability (and monotonicity) by using {\tt exhaustive} scenario.
In {\tt exhaustive}, multiple query executions are made at each cardinality, 
varying the max cardinality to the min one; namely, for a query we get a plan 
and execute the query multiple times, saying 10, upon decreasing the cardinality. 
Instead of the incremental 10K-row-deletion, 
changing the cardinality is done by copying from the max table as many rows as needed 
so that all pages are in full with the rows. 
This setup allows us to check if any of the multiple executions produces 
different plans at the same cardinality (and see if the query execution 
times are monotonically decreasing as cardinality decreases). 
Each DBMS ran {\tt exhaustive} with a run of 10 queries, and it had 
no case such that different plans were detected at the same cardinality 
while a query got executed repeatedly.
}
