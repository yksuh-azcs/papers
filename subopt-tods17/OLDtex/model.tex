\subsection{A Model of Suboptimality}
\todo{Sabah has the modify token}
The model suggests how some factors may impact the prevalence of
suboptimality. (In preliminary investigation, we have seen that
different queries are suboptimal in different DBMSes, and so it is
more productive to focus on the prevalence of suboptimality than on
the harder, and perhaps less illuminative, task of predicting which
individual queries produce suboptimal plans.) We present our model in
a hierarchical manner such that the dependencies among the components
can be directly revealed. We partition the factors in our model into
two categories, namely external and internal factors. ``Query
complexity'' is thus an external factor, in that they are not part of
the query optimizer. We can manipulate the external factors in the
experiment. Query complexity includes such factors as the number of
correlation names in the from clause, the presence of aggregates, the
number of self joins. The number of correlation names in the from
clause and the presence of aggregates will weakly influence
suboptimality directly. Instead, all the Query complexity
instantiations will influence suboptimality indirectly by influencing
plan space complexity. The number of correlation names in the from
clause will exponentially influence plan space complexity. The
presence of aggregates will linearly influence plan space complexity.
The number of self joins will negatively correlate with plan space
complexity. Structure of the tables used by the queries is another
external factor called ``Schema complexity''. Schema complexity
include factors like presence of primary keys and indexes.

[Is the sentence below accurate?]
Presence of primary keys, or indexes if present on columns involved in
the join condition between tables involved in a query, will reduce the
plan space complexity and thus inderectly reduce suboptimality present
in the query. [What is the function of the relationship between
presence of indexes and CEPS / Suboptimality]

Concerning the internal factors, some are independent variables, which
we can control, and some are {\em latent variables}, which are derived
from independent variables or which are internal to the optimizer. The
number of aggregates is an independent variable.

One of the central causal factors of this model is the optimizer
complexity. The model states that if the optimizer is very complex,
i.e., if the optimizer has to contend with a large number
of available operators, that will also increase the number of
interactions, and thus the
prevalence of suboptimal queries.

As we cannot observe the plan space complexity of the optimizer
directly, we include in the model other components, which may, from
different angles, indirectly help us understand the effects of the
complexity of the optimizer. Given a particular query, its measurable
complexity will impact the total number of candidate plans considered
by the optimizer. However, this latter factor is not directly
observable, again, especially for propriety systems. However, we {\em
can} measure the number of plans actually generated by the optimizer
when presented with different
cardinalities of the underlying tables. We term this set of plans
actually generated the
``effective plan space'' and the number of such plans, the
``cardinality of the effective plan
space,'' or {\em CEPS}. The model states a correlation between the
latent variable of the number
of plans considered and the measurable dependent variable of CEPS. The
model also states a
positive correlation between the number of operators available in the
DBMS and plan space
complexity. Another feature of the model is the positive correlation
between the number of
operators available in the DBMS and the number of operators observed
for each functionality. Only a subset of the physical operators
available will actually be considered depending on the
presence of a particular functionality (such as join) in the query.

We have just {\em described} how suboptimality might arise, through a
theory and its elaborated
model. We now discuss {\em prediction}.
How might we test such a theory? We hypothesize from our model that as
the optimizer becomes increasingly complex, more occurrences of
suboptimality will be observed, because of such unintended
interactions. Similarly, with a more simplistic optimizer, we
hypothesize that there will be fewer unanticipated interactions, and
so less suboptimality would be present. We predict that one source of
the suboptimality is the designer's focus on correctness and
performance, making it difficult to predict subtle interactions
between components of the optimizer. Later we explore the
ramifications of this hypothesis, finding support for our proposed
model.

\todo{Questions:}
1. I presume the following should go in section 1/ 4, since they are
related to how we
operationalized / tested hypotheses
- heuristic nature of some DBMSes
- Daemons and how we controlled for them

2. Do we still think that the different components of query
complexity, schema complexity and
optimizer complexity will influence suboptimality through CEPS? If so,
why didn't we see the
relationship last time? Or is the relationship between the independent
variables through cost
model accuracy (or some other factor) that we haven't operationalized yet?

3. How does primary keys / indexes impact suboptimality?


\todo{ operationalization def, high level}
Once a particular query is determined to be suboptimal, the obvious
next step would be to look at the code of the query optimizer (if
indeed the code is even available) and alter it in some way so that it
to reduce suboptimality. What we have done instead is to approach this
phenomenon from a scientific perspective, attempting to move to the
right on the $x$ axis of Figure~\ref{fig:empirical}, by developing a
simple predictive model of suboptimality, shown in
Figure~\ref{fig:model}.

The purpose of query optimization is to generate optimal plans.  So
why would suboptimality occur in the first place? Query optimizers are
highly complex, comprised of tens of thousands of lines of code. There
are several reasons for this complexity. First, an optimizer must
contend with the richness of the query language, SQL, whose definition
requires about 2000 pages~\cite{SQL2008}, with a multiple of
linguistic features. Second, an optimizer must content with the
richness of the physical operators available to it. DBMSes have a
range of algorithms available to evaluate each of many algebraic
operators. Third, the optimizer must contend with an exponential
number of query evaluation plans. Kabra and DeWitt~\cite{kabra98}
identify several other sources of complexity: inaccurate statistics on
the underlying tables and insufficient information about the runtime
system: ``amount of available resources (especially memory), the load
on the system, and the values of host language
variables.''~(page~106). They also mention user-defined data types,
methods, and operators allowed by the newer object-relational
systems~\cite{Melton03}. Thus, the task of optimization is very
complex, with the result that the optimizers themselves consist of a
collection of ``components,'' that is, the rules or heuristics that it
uses during optimization, with each of these components being itself
complex.

Our theory, which we will investigate in some detail and which is the
basis for our predictive model, is that suboptimality is due in part
to the complexity of the optimizer and concomitant unanticipated
interactions between the different components used within the 
optimizer.  We argue that with the proliferation of concerns and
variability that an optimizer must contend with, it is extremely
difficult to ensure that for an arbitrary query that the optimizer
will {\em always} pick the right plan. The components of the optimizer
are all interacting with each other; our theory implicate these {\em
interactions} as one source of suboptimality.

The model of Figure~\ref{fig:model} suggests how some factors may
impact the prevalence of suboptimality. (In preliminary investigation,
we have seen that different queries are suboptimal in different
DBMSes, and so it is more productive to focus on the prevalence of
suboptimality than on the harder, and perhaps less illuminative, task
of predicting which individual queries produce suboptimal plans.) We
present our model in a hierarchical manner such that the dependencies
among the components can be directly revealed. We partition the
factors in our model into two categories, namely external and internal
factors. ``Query complexity'' is thus an external factor, in that it is 
not part of the query optimizer. We can manipulate the external 
factors in the experiment. Query complexity includes such factors as
the number of correlation names in the from clause, the presence of
aggregates, the number of self joins. The number of correlation names
in the from clause and the presence of aggregates will weakly
influence suboptimality directly. Instead, all the Query complexity
instantiations will influence suboptimality indirectly by influencing
plan space complexity. The number of correlation names in the from
clause will exponentially influence plan space complexity. The
presence of aggregates will linearly influence plan space complexity.
The number of self joins will negatively correlate with plan space
complexity.
Therefore, our hypotheses with respect to the impact of Query Complexity are
Hypothesis 1: The number of correlation names will exponentially influence
Plan space complexity.
Hypothesis 2: The presence of aggregates will linearly influence plan space
complexity.
Hypothesis 3: The number of self joins will negatively correlate with
plan space complexity. 

Concerning the internal factors, some are independent variables, which
we can control, and some are {\em latent variables}, which are derived
from independent variables or which are internal to the optimizer. The
number of aggregates is an independent variable.

One of the central causal factors of this model is the optimizer
complexity. The model states that if the optimizer is very complex,
i.e., if the optimizer has to contend with a large number of available
operators, that will also increase the number of interactions, and
thus the prevalence of suboptimal queries.
Hypothesis 4: As the number of operators increase, the prevalence of suboptimality will increase.

As we cannot observe the plan space complexity of the optimizer
directly, we include in the model other components, which may, from
different angles, indirectly help us understand the effects of the
complexity of the optimizer. Given a particular query, its measurable
complexity will impact the total number of candidate plans considered
by the optimizer. However, this latter factor is not directly
observable, again, especially for propriety systems. However, we {\em
can} measure the number of plans actually generated by the optimizer
when presented with different cardinalities of the underlying tables.
We term this set of plans actually generated the ``effective plan
space'' and the number of such plans, the ``cardinality of the
effective plan space,'' or {\em CEPS}. The model states a correlation
between the latent variable of the number of plans considered and the
measurable dependent variable of CEPS. The model also states a
positive correlation between the number of operators available in the
DBMS and plan space complexity. Another feature of the model is the
positive correlation between the number of operators available in the
DBMS and the number of operators observed for each functionality. Only
a subset of the physical operators available will actually be
considered depending on the presence of a particular functionality
(such as join) in the query.
Hypothesis 4: Plan space complexity (CEPS) linearly influences suboptimality.

\todo{Add a few sentences about number of operators observed across functionality. }
The types of functionality involved vary based on the type of SQL query. 
For each functionality, the DBMS may include multiple different physical operators
to implement the functionality. The main
types of functionality included in this study involve joins, and aggregates. 
Within joins, we also looked at the impact of self joins. Optimizers have
different operators for each functionality. For example, if the query did 
not involve joins, none of the join operators would be considered and thus
they would not contribute to the complexity for that query. Although we cannot
extract the operators that were considered for proprietary DBMSes. However, 
we can approximate this by observing the operators that were selected 
across different queries sharing functionality. And, that is what we did
to measure operators.

In the characterization of the $x$ axis of Figure~\ref{fig:empirical}
of empirical generalization, we have just {\em described} how
suboptimality might arise, through a theory and its elaborated model.
We now need to move right on that axis, to {\em prediction}.

How might we test such a theory? We hypothesize from our model that as
the optimizer becomes increasingly complex, more occurrences of
suboptimality will be observed, because of such unintended
interactions. Similarly, with a more simplistic optimizer, we
hypothesize that there will be fewer unanticipated interactions, and
so less suboptimality would be present. We predict that one source of
the suboptimality is the designer's focus on correctness and
performance, making it difficult to predict subtle interactions
between components of the optimizer. Later we explore the
ramifications of this hypothesis, finding support for our proposed
model.
\todo{Also add details why our model is necessary before examining data
skew, indexes, etc. }

